{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98df1c41-e588-453d-924e-5daeddb37b3a",
   "metadata": {},
   "source": [
    "## Clothings Inventory And Sorting with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a59a003-d9a8-4c5d-b29b-fcea944f513e",
   "metadata": {},
   "source": [
    "**INTRODUCTION**\n",
    "\n",
    "*Project Overview*\n",
    "- This project involves building a machine learning model that uses Convolutional Neural Network (CNN) to identify different inventory products and categorise them into specific classes where summary of the total inventory can be extracted. The processes involved in setting up the model includes data collection, cleaning, importing and installation of the needed libraries, setting parameters, data preprocessing and augmentation, model definition, compilation, training, evaluation and testing.\n",
    "\n",
    "*Problem Statement*\n",
    "- Overtime sorting and taking inventory has always been a little challenging and somewhat exhausting sometimes, engaging in both physical and mental exercises which in some cases might require repetition for accuracy. A a solution, this setup with the trained model helps to identify each product according to its category and provides a summary of available product given the pictorial data of all the products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a437e-c5af-4d56-9a1f-52255895c0d0",
   "metadata": {},
   "source": [
    "#### Importing librabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e58c5c3-d3c9-488a-ab8c-d4cee8b5787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1f715c-b66f-410f-a4f3-08a0a89fbf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41b6cd-2f4e-499f-89db-257da42b4010",
   "metadata": {},
   "source": [
    "#### Defining image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0ea229-b6d5-4e27-aadd-d40f8733435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 64, 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdd545-1164-43e5-9112-498d62986290",
   "metadata": {},
   "source": [
    "#### Defining categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36387ec-87eb-4f9b-9bef-dbc57b3534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Dress', 'Pants', 'Shirt', 'Shoes'] \n",
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65409c2-bc04-415d-95fb-728dbfa7826a",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7c1b1-babd-45df-ba8f-1959ac6d3c22",
   "metadata": {},
   "source": [
    "#### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff525372-70d5-4d89-8e36-f2eb1a74756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2556 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    "    )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set', \n",
    "    target_size=(img_height, img_width), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018eb04-7107-4dc4-be41-4ffcbc15e67a",
   "metadata": {},
   "source": [
    "#### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4c0632-8a96-4a47-8d87-b7492fe57884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set', \n",
    "    target_size=(img_height, img_width), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1e46f-b643-446f-805a-81b901520854",
   "metadata": {},
   "source": [
    "#### CNN model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33751020-c549-4a48-bff1-15cb6733eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(img_height,img_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=1))\n",
    "\n",
    "# Convolutional layer 2\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "# Convolutional layer 3\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604bb0de-0f40-4404-9e8d-73346509f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "2556\n",
      "{'dress': 0, 'pants': 1, 'shirts': 2, 'shoes': 3}\n",
      "{'dress': 0, 'pants': 1, 'shirts': 2, 'shoes': 3}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.samples)\n",
    "print(train_generator.samples)\n",
    "print(validation_generator.class_indices)\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac58dd5-d4d7-4623-af8f-ba2eca3f17ef",
   "metadata": {},
   "source": [
    "#### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f74b7299-d383-430c-8299-fa6b265b7834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <Sequential name=sequential, built=True>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc87ef-ce9c-4f04-a5ee-7c0cc196cbc0",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bcd40fa-2b04-455d-a614-81266f463d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 449ms/step - accuracy: 0.5869 - loss: 1.0310 - val_accuracy: 0.6750 - val_loss: 0.8089\n",
      "Epoch 2/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 438ms/step - accuracy: 0.7269 - loss: 0.7267 - val_accuracy: 0.7500 - val_loss: 0.6106\n",
      "Epoch 3/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 445ms/step - accuracy: 0.7778 - loss: 0.6103 - val_accuracy: 0.8188 - val_loss: 0.4807\n",
      "Epoch 4/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 440ms/step - accuracy: 0.8016 - loss: 0.5335 - val_accuracy: 0.8313 - val_loss: 0.4500\n",
      "Epoch 5/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 447ms/step - accuracy: 0.8247 - loss: 0.4785 - val_accuracy: 0.8500 - val_loss: 0.4537\n",
      "Epoch 6/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 453ms/step - accuracy: 0.8490 - loss: 0.4367 - val_accuracy: 0.8750 - val_loss: 0.3777\n",
      "Epoch 7/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 446ms/step - accuracy: 0.8549 - loss: 0.3890 - val_accuracy: 0.9000 - val_loss: 0.3306\n",
      "Epoch 8/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 438ms/step - accuracy: 0.8599 - loss: 0.3794 - val_accuracy: 0.8938 - val_loss: 0.3235\n",
      "Epoch 9/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 449ms/step - accuracy: 0.8858 - loss: 0.3185 - val_accuracy: 0.8813 - val_loss: 0.3120\n",
      "Epoch 10/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 442ms/step - accuracy: 0.8948 - loss: 0.2914 - val_accuracy: 0.9250 - val_loss: 0.2196\n",
      "Epoch 11/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 446ms/step - accuracy: 0.9135 - loss: 0.2533 - val_accuracy: 0.9438 - val_loss: 0.1908\n",
      "Epoch 12/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 452ms/step - accuracy: 0.9151 - loss: 0.2468 - val_accuracy: 0.9375 - val_loss: 0.2323\n",
      "Epoch 13/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 451ms/step - accuracy: 0.9178 - loss: 0.2215 - val_accuracy: 0.9375 - val_loss: 0.2968\n",
      "Epoch 14/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 451ms/step - accuracy: 0.9327 - loss: 0.1855 - val_accuracy: 0.9438 - val_loss: 0.2073\n",
      "Epoch 15/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 431ms/step - accuracy: 0.9366 - loss: 0.1791 - val_accuracy: 0.9500 - val_loss: 0.1391\n",
      "Epoch 16/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 453ms/step - accuracy: 0.9417 - loss: 0.1581 - val_accuracy: 0.9500 - val_loss: 0.2458\n",
      "Epoch 17/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 441ms/step - accuracy: 0.9468 - loss: 0.1624 - val_accuracy: 0.9500 - val_loss: 0.2416\n",
      "Epoch 18/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 446ms/step - accuracy: 0.9515 - loss: 0.1474 - val_accuracy: 0.9688 - val_loss: 0.1774\n",
      "Epoch 19/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 444ms/step - accuracy: 0.9566 - loss: 0.1312 - val_accuracy: 0.9812 - val_loss: 0.1499\n",
      "Epoch 20/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 450ms/step - accuracy: 0.9589 - loss: 0.1239 - val_accuracy: 0.9812 - val_loss: 0.0642\n",
      "Epoch 21/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 446ms/step - accuracy: 0.9640 - loss: 0.1119 - val_accuracy: 0.9688 - val_loss: 0.1957\n",
      "Epoch 22/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 444ms/step - accuracy: 0.9538 - loss: 0.1392 - val_accuracy: 0.9750 - val_loss: 0.1153\n",
      "Epoch 23/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 446ms/step - accuracy: 0.9675 - loss: 0.0932 - val_accuracy: 0.9187 - val_loss: 0.2244\n",
      "Epoch 24/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 447ms/step - accuracy: 0.9667 - loss: 0.0902 - val_accuracy: 0.9563 - val_loss: 0.1695\n",
      "Epoch 25/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 441ms/step - accuracy: 0.9703 - loss: 0.0883 - val_accuracy: 0.9688 - val_loss: 0.1119\n",
      "Epoch 26/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 456ms/step - accuracy: 0.9687 - loss: 0.0902 - val_accuracy: 0.9500 - val_loss: 0.2002\n",
      "Epoch 27/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 437ms/step - accuracy: 0.9656 - loss: 0.1010 - val_accuracy: 0.9250 - val_loss: 0.2555\n",
      "Epoch 28/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 453ms/step - accuracy: 0.9660 - loss: 0.0944 - val_accuracy: 0.9625 - val_loss: 0.1418\n",
      "Epoch 29/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 452ms/step - accuracy: 0.9750 - loss: 0.0769 - val_accuracy: 0.9625 - val_loss: 0.1561\n",
      "Epoch 30/30\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 451ms/step - accuracy: 0.9777 - loss: 0.0675 - val_accuracy: 0.9688 - val_loss: 0.1037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24ce6cba120>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_generator, validation_data=validation_generator, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a79b28-e539-44dc-bd6b-056c1a93532d",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e045ac4-9335-4a2e-8050-570290e869b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9688 - loss: 0.1037\n",
      "Validation loss: 0.1, Validation Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation loss: {round(loss, 2)}, Validation Accuracy: {round(accuracy, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c71f4-f812-4100-a49f-5cc42d5466f7",
   "metadata": {},
   "source": [
    "### Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666e2fb3-fd9a-4993-a869-77e4aa6e0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "Dress\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "categories = ['Dress', 'Pant', 'Shirt', 'Shoe'] \n",
    "\n",
    "test_image = image.load_img('test/dress (4).jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis= 0)\n",
    "result = model.predict(test_image)\n",
    "train_generator.class_indices\n",
    "\n",
    "prediction = categories[np.argmax(result)]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7b714-4c8c-4f13-929a-c0134a6125ec",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb74a2c-b94c-42fe-b830-8a078cafdc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('models/clothing_cnn_model_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46466f9f-c300-4ebe-a406-bf6cad3df669",
   "metadata": {},
   "source": [
    "#### Reusing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98c3c15-198c-4551-9199-bf4c25949ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Define the categories\n",
    "categories = ['Dress', 'Pant', 'Shirt', 'Shoe'] \n",
    "\n",
    "# Load the save model\n",
    "model = load_model('models/clothing_cnn_model_v2.h5')\n",
    "\n",
    "# Function for image prediction\n",
    "def predict_category(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)/255.0\n",
    "    \n",
    "    # Predict the class probabilities\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    category = categories[predicted_class]\n",
    "\n",
    "    # print(f\"Predicted category: {category}\")\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0153bd-db3c-4740-ad3a-9d14e96c6353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Inventory Summary: \n",
      "Dresses: 10 \n",
      "Pants: 14 \n",
      "Shirts: 9 \n",
      "Shoes: 11 \n",
      "Unidentified: 0\n",
      "\n",
      "Item categories and file name\n",
      "Dresses: ['dress (10).jpg', 'dress (11).jpg', 'dress (12).jpg', 'dress (13).jpg', 'dress (14).jpg', 'dress (15).jpg', 'dress (16).jpg', 'dress (17).jpg', 'dress (18).jpg', 'dress (4).jpg'] \n",
      "\n",
      "Pants: ['pant-checkpoint.jpeg', 'pant.jpeg', 'pants (1).jpg', 'pants (10).jpg', 'pants (11).jpg', 'pants (12).jpg', 'pants (2).jpg', 'pants (3).jpg', 'pants (4).jpg', 'pants (5).jpg', 'pants (6).jpg', 'pants (7).jpg', 'pants (8).jpg', 'pants (9).jpg'] \n",
      "\n",
      "Shirts: ['shirt-checkpoint.jpeg', 'shirt.jpeg', 'shirts (18).jpg', 'shirts (19).jpg', 'shirts (20).jpg', 'shirts (21).jpg', 'shirts (22).jpg', 'shirts (23).jpg', 'shirts (24).jpg'] \n",
      "\n",
      "Shoes: ['shoes (1).jpg', 'shoes (10).jpg', 'shoes (11).jpg', 'shoes (2).jpg', 'shoes (3).jpg', 'shoes (4).jpg', 'shoes (5).jpg', 'shoes (6).jpg', 'shoes (7).jpg', 'shoes (8).jpg', 'shoes (9).jpg']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Test prediction\n",
    "dress = []\n",
    "shoes = []\n",
    "pants = []\n",
    "shirts = []\n",
    "unidentified = []\n",
    "\n",
    "folder = Path(\"C:/Users/fortu/Desktop/Programming/Projects/Clothings Inventory Sorting App/test\")\n",
    "\n",
    "fileNames = [file.name for file in folder.iterdir() if file.is_file()]\n",
    "\n",
    "for i in fileNames:\n",
    "    file_path = f'{folder}/{i}'\n",
    "    prediction = predict_category(file_path)\n",
    "    if prediction == 'Dress':\n",
    "        dress.append(i)\n",
    "    elif prediction == 'Shoe':\n",
    "        shoes.append(i)\n",
    "    elif prediction == 'Pant':\n",
    "        pants.append(i)\n",
    "    elif prediction == 'Shirt':\n",
    "        shirts.append(i)\n",
    "    else:\n",
    "        unidentified.append(i)\n",
    "\n",
    "\n",
    "output = f\"Dresses: {len(dress)} \\nPants: {len(pants)} \\nShirts: {len(shirts)} \\nShoes: {len(shoes)} \\nUnidentified: {len(unidentified)}\"\n",
    "print(f\"Inventory Summary: \\n{output}\")\n",
    "\n",
    "print(\"\\nItem categories and file name\")\n",
    "print(f\"Dresses: {dress} \\n\\nPants: {pants} \\n\\nShirts: {shirts} \\n\\nShoes: {shoes}\")\n",
    "if len(unidentified) > 0:\n",
    "    print(f\"Unidentifed: {unidentified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbe117-48f7-45a5-ad2e-efcbcd6bd130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
